# -*- coding: utf-8 -*-
"""Selection of Machine Learning Algorithms.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WF8I_kVAvK8BI0l8DvUYMaOZ1VuCQBe0
"""

!pip install feature_engine
!pip install lightgbm

!pip install optuna

pip install optuna-integration

import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LogisticRegression
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from sklearn.model_selection import train_test_split
from sklearn.metrics import balanced_accuracy_score, classification_report

from feature_engine.outliers import OutlierTrimmer
from feature_engine.encoding import RareLabelEncoder, MeanEncoder, WoEEncoder, OrdinalEncoder, DecisionTreeEncoder
from feature_engine.discretisation import DecisionTreeDiscretiser
from feature_engine.pipeline import Pipeline
from feature_engine.wrappers import SklearnTransformerWrapper
from feature_engine.imputation import CategoricalImputer

import optuna
from optuna.integration import LightGBMPruningCallback

import warnings
warnings.filterwarnings('ignore')

df = pd.read_parquet('/content/base_df.pq').dropna(subset='TARGET').set_index('SK_ID_CURR', drop=True)
df['NAME_FAMILY_STATUS'] = df['NAME_FAMILY_STATUS'].replace('Unknown', 'Single / not married')
df['AMT_REQ_CREDIT_BUREAU_MON'] = df['AMT_REQ_CREDIT_BUREAU_MON'].astype('O')

categorical = df.select_dtypes(include=['object']).columns.tolist()
numerical = df.select_dtypes(include=['number']).columns.tolist()

knn_impute = SklearnTransformerWrapper(KNNImputer())
pipe = Pipeline(steps=[
    ('Cat_imputer', CategoricalImputer(variables=categorical, fill_value='Unknown / None')),
    ('Rare_labels', RareLabelEncoder(tol=0.01, n_categories=5, variables=categorical)),
    ('tree_cat_encoder', DecisionTreeEncoder(regression=False, scoring='completeness_score')),
    ('impute_nan', knn_impute),
    #('outlier_removal', OutlierTrimmer('gaussian', fold=3, variables=numerical))
])

processed_df = pipe.fit_transform(df, df['TARGET'])

X = processed_df.drop(columns=['TARGET'])
y = processed_df['TARGET']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

def objective_lr(trial):
    params = {
        'C': trial.suggest_loguniform('C', 1e-4, 1e2),
        'solver': trial.suggest_categorical('solver', ['liblinear', 'lbfgs']),
    }
    model = LogisticRegression(**params)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    return balanced_accuracy_score(y_test, y_pred)

def objective_rf(trial):
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 50, 300),
        'max_depth': trial.suggest_int('max_depth', 2, 32, log=True),
        'min_samples_split': trial.suggest_int('min_samples_split', 2, 14),
        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 14),
    }
    model = RandomForestClassifier(**params)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    return balanced_accuracy_score(y_test, y_pred)

def objective_svm(trial):
    params = {
        'C': trial.suggest_loguniform('C', 1e-4, 1e2),
        'kernel': trial.suggest_categorical('kernel', ['linear', 'poly', 'rbf', 'sigmoid']),
    }
    model = SVC(**params)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    return balanced_accuracy_score(y_test, y_pred)

def objective_xgb(trial):
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 50, 300),
        'max_depth': trial.suggest_int('max_depth', 2, 32, log=True),
        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-4, 1e-1),
        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),
        'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),
    }
    model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', **params)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    return balanced_accuracy_score(y_test, y_pred)

def objective_lgbm(trial):
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 50, 300),
        'max_depth': trial.suggest_int('max_depth', 2, 32, log=True),
        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-4, 1e-1),
        'num_leaves': trial.suggest_int('num_leaves', 2, 256, log=True),
        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.5, 1.0),
    }
    model = LGBMClassifier(**params)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    return balanced_accuracy_score(y_test, y_pred)

# Run Optuna studies for each model
study_lr = optuna.create_study(direction='maximize')
study_lr.optimize(objective_lr, n_trials=50)
print("Best hyperparameters for Logistic Regression:", study_lr.best_params)

study_rf = optuna.create_study(direction='maximize')
study_rf.optimize(objective_rf, n_trials=50)
print("Best hyperparameters for Random Forest:", study_rf.best_params)

study_svm = optuna.create_study(direction='maximize')
study_svm.optimize(objective_svm, n_trials=50)
print("Best hyperparameters for SVM:", study_svm.best_params)

study_xgb = optuna.create_study(direction='maximize')
study_xgb.optimize(objective_xgb, n_trials=50)
print("Best hyperparameters for XGBoost:", study_xgb.best_params)

study_lgbm = optuna.create_study(direction='maximize')
study_lgbm.optimize(objective_lgbm, n_trials=50)
print("Best hyperparameters for LightGBM:", study_lgbm.best_params)

# Logistic Regression
lr_model = LogisticRegression(**study_lr.best_params)
lr_model.fit(X_train, y_train)
print("Logistic Regression Evaluation:")
evaluate_model(lr_model, X_test, y_test)

# Random Forest
rf_model = RandomForestClassifier(**study_rf.best_params)
rf_model.fit(X_train, y_train)
print("Random Forest Evaluation:")
evaluate_model(rf_model, X_test, y_test)

# Support Vector Machine
svm_model = SVC(**study_svm.best_params)
svm_model.fit(X_train, y_train)
print("SVM Evaluation:")
evaluate_model(svm_model, X_test, y_test)

# XGBoost
xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', **study_xgb.best_params)
xgb_model.fit(X_train, y_train)
print("XGBoost Evaluation:")
evaluate_model(xgb_model, X_test, y_test)

# LightGBM
lgbm_model = LGBMClassifier(**study_lgbm.best_params)
lgbm_model.fit(X_train, y_train)
print("LightGBM Evaluation:")
evaluate_model(lgbm_model, X_test, y_test)

# Basic Neural Network
nn_model = Sequential()
nn_model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))
nn_model.add(Dense(32, activation='relu'))
nn_model.add(Dense(1, activation='sigmoid'))

nn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
nn_model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1)
nn_loss, nn_accuracy = nn_model.evaluate(X_test, y_test)
print(f"Neural Network Accuracy: {nn_accuracy}")